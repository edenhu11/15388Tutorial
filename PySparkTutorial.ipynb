{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations for using Spark\n",
    "\n",
    "Apache Spark is a cluster computing platform designed to be fast on rather large datasets. \n",
    "\n",
    "Data sizes are growing these days, and we need to be able to process and conduct analysis on big data using reasonable amount of time. Spark offers the opportunity to do that due to its unique ability to process data in memory. This advantage of Spark especially shows itself when doing innately iterative machine learning tasks. This is different from its predecessors like Hadoop which does processing in disk.\n",
    "\n",
    "More practically speaking, knowing how to use Spark is definitely a plus on your resume as a data analyst, data scientist or data engineer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Spark Works\n",
    "\n",
    "Spark operates on the idea of distributed data parallelism. We split the data over several working nodes, which independently operate on the data shards in parallel. If necessary, the data shards are then combined afterwards. For this tutorial, we will not worry about the particular inner structure of Spark. In fact, you do not need to know the details of what happens under the hood to be able to use basic spark!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Content\n",
    "\n",
    "In this tutorial, we will cover the basics of PySpark and the machine learning package in PySpark. \n",
    "\n",
    "* [Downloading and Installing PySpark](#Downloading-and-Installing-PySpark)\n",
    "* [Starting our first SparkContext and SparkSession](#Starting-our-first-SparkContext-and-SparkSession)\n",
    "* [Creating or Loading Sample Data](#Creating-or-Loading-Sample-Data)\n",
    "* [RDD and DataFrame Basics](#RDD-and-DataFrame-Basics)\n",
    "* [Machine Learning in PySpark](#Machine-Learning-in-PySpark)\n",
    "* [Shutting off a SparkContext](#Shutting-off-a-SparkContext)\n",
    "* [PySpark in Action: Will I Respond Correctly?](#PySpark-in-Action:-Will-I-Respond-Correctly?)\n",
    "* [Starting a Spark Cluster](#Starting-a-Spark-Cluster)\n",
    "* [Useful Resources](#Useful-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Installing PySpark\n",
    "\n",
    "First, if you do not already have PySpark, install it using pip\n",
    "\n",
    "    $ pip install pyspark \n",
    "\n",
    "After installing pySpark, make sure the following command works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting our first SparkContext and SparkSession\n",
    "\n",
    "SparkContext is an entry point to Spark programming with RDD and to connect to Spark Cluster. Since Spark 2.0 SparkSession has been introduced and became an entry point to start programming with DataFrame. Don't worry if terms like RDD and DataFrame confuse you now. We will formally introduce them in our next section. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating or Loading Sample Data\n",
    "\n",
    "Before getting to the details of creating/loading data, let us sart with an introduction to PySpark's data structures: RDDs and DataFrames:\n",
    "\n",
    "#### RDDs (Resilient Distributed Datasets)\n",
    "RDDs are the elements in PySpark that run and operate on multiple nodes to do parallel processing. The two key features of RDDs are immutable and fault tolerant. Once you create a RDD, you cannot change it in any way. This allows for fault tolerance, since if any process fails, it can potentially restart with the original (unchanged) data. \n",
    "\n",
    "#### DataFrames\n",
    "DataFrames are SQL-like structured datasets with query operations. It is a relational API over RDDs. In contrast to RDDs being a functional API, DataFrames is a declarative relational API. \n",
    "\n",
    "Rule of Thumb: If data permits it, we should almost always use DataFrame as it providers a higher level of abstraction with optimization potentials and thus performance benefits. Below, we introduce creating and importing data as RDDs and Dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a simple RDD using SparkContext's parallelize on a existing iterable or collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', [10405, 15213, 10301]),\n",
       " ('Bob', [10405, 10701]),\n",
       " ('Chad', [15513, 15445, 10405])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a list of (name,list(courses))\n",
    "l = [('Alice', [10405, 15213, 10301]),('Bob', [10405, 10701]),('Chad', [15513, 15445, 10405])]\n",
    "l_rdd = sc.parallelize(l)\n",
    "l_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a RDD from an external dataset \n",
    "\n",
    "We obtained the datatset from a course I have taken (85-432) and modified and cleaned it for this tutorial. If you want to follow along, please download the dataset here: [joined_Lexical_Data.csv](https://github.com/edenhu11/15388Tutorial/blob/main/joined_Lexical_Data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"X\",\"Sub_ID\",\"Trial\",\"Type\",\"D_RT\",\"D_Word\",\"Outlier\",\"D_Zscore\",\"Correct\",\"Length\",\"Log_Freq_HAL\"',\n",
       " '1,157,1,1,\"710\",\"browse\",\"true\",-0.437,1,6,8.856',\n",
       " '2,67,1,1,\"1,094\",\"refrigerant\",\"false\",0.825,1,11,4.644',\n",
       " '3,120,1,1,\"587\",\"gaining\",\"false\",-0.645,1,7,8.304',\n",
       " '4,21,1,1,\"984\",\"cheerless\",\"true\",0.025,1,9,2.639']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_rdd = sc.textFile('joined_Lexical_Data.csv')\n",
    "ld_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a dataframe with createDataFrame() using an RDD, a list or a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| name|             courses|\n",
      "+-----+--------------------+\n",
      "|Alice|[10405, 15213, 10...|\n",
      "|  Bob|      [10405, 10701]|\n",
      "| Chad|[15513, 15445, 10...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(l,['name','courses'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a dataframe from an external dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----+-----+-----------+-------+--------+-------+\n",
      "|_c0|Sub_ID|Trial|Type| D_RT|     D_Word|Outlier|D_Zscore|Correct|\n",
      "+---+------+-----+----+-----+-----------+-------+--------+-------+\n",
      "|  1|   157|    1|   1|  710|     browse|  false|  -0.437|      1|\n",
      "|  2|    67|    1|   1|1,094|refrigerant|  false|   0.825|      1|\n",
      "|  3|   120|    1|   1|  587|    gaining|  false|  -0.645|      1|\n",
      "+---+------+-----+----+-----+-----------+-------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import csv, tell the function that the csv has a header and ask it to infer that data type of each column.\n",
    "ld_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"LexicalData.csv\")\n",
    "ld_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD and DataFrame Basics\n",
    "\n",
    "### RDD\n",
    "For RDD only, there are two types of operations: transformation and action. Transformations always happen on a previous RDD and then return a newly changed RDD. In contrast, an action returns something other than a RDD, such an integer. \n",
    "\n",
    "Another very important distinction is that all transformations are lazy, as they are not really exectuted when you run them. They are merely \"planned\" to be executed. All the transformations are really executed when an action is called, which as opposed to lazy, is eager. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Transformation Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter(): \n",
    "use ```filter()``` text to RDD element with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', [10405, 15213, 10301])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_rdd = l_rdd.filter(lambda x: \"Alice\" in x)\n",
    "alice_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### union() \n",
    "use ```union()``` combines two RDD together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', [10405, 10701]), ('Alice', [10405, 15213, 10301])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rdd = l_rdd.filter(lambda x: \"Bob\" in x).union(alice_rdd)\n",
    "combined_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### map()\n",
    "use ```map()``` to do operation on every element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', [15213, 10301]), ('Bob', [10701]), ('Chad', [15445, 10405])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rid of everyone's first class in the list\n",
    "l_rdd_new = l_rdd.map(lambda x:(x[0],x[1][1:]))\n",
    "l_rdd_new.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reduceByKey()\n",
    "use ```reduceByKey()``` to calculate a value for the RDD elements in the same group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 3), ('Bob', 5), ('Chad', 6)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [('Alice',3),('Bob',5),('Chad',6)]\n",
    "l_rdd2 = sc.parallelize(l2)\n",
    "l_rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', (3, 1)), ('people', (5, 1)), ('people', (6, 1))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduceByKey to calculate average. \n",
    "l_rdd2_remapped = l_rdd2.map(lambda x:('people',(x[1],1))) # map all people to a common group \"people\"\n",
    "l_rdd2_remapped.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', (14, 3))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = l_rdd2_remapped.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])) # calculate the sum\n",
    "average.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.666666666666667]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average.map(lambda x:x[1][0]/x[1][1]).take(1) # calcualte average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Action Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collect()\n",
    "It returns all the elements of the dataset as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 3), ('Bob', 5), ('Chad', 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count() and countByValue()\n",
    "we use ```count()``` to count the number of lines in a RDD. ```countByValue()``` counts the occurence of each RDD element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('Alice', 3): 1, ('Bob', 5): 1, ('Chad', 6): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rdd2.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce()\n",
    "```reduce``` aggragate the elements using the provided function and returns a value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate average using reduce\n",
    "#first, get the age as a separate RDD\n",
    "sum_rdd = l_rdd2.map(lambda x:x[1])\n",
    "sum_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map each element to one to calculate number of elements\n",
    "num_rdd = l_rdd2.map(lambda x:(1))\n",
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum is:  14\n",
      "number of element is:  3\n",
      "average is:  4.666666666666667\n"
     ]
    }
   ],
   "source": [
    "#calculate average\n",
    "sumVal = sum_rdd.reduce(lambda accum,n: accum +n)\n",
    "print(\"sum is: \",sumVal)\n",
    "numVal = num_rdd.reduce(lambda accum,n: accum +n)\n",
    "print(\"number of element is: \",numVal)\n",
    "avg = sumVal/numVal\n",
    "print(\"average is: \",avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introduction is surely not exhausitive, so feel free to check the Spark API for more examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "Spark DataFrames are very similar to Pandas DataFrame. Read the [PySpark in Action: Will I Respond Correctly?](#PySpark-in-Action:-Will-I-Respond-Correctly?) section for a quick example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in PySpark\n",
    "\n",
    "PySpark provides a general machine learning library (MLlib). \n",
    "In this section, we will introduce two of the most common techniques to pre-process categroical data: StringIndexer and OneHotEncoder. Then, we go into Spark's machine learning pipeline.\n",
    "\n",
    "### ```StringIndexer``` and ```OneHotEncoderEstimator```\n",
    "In Spark, ```Stringindexer``` maps a categorical variable column to an index column, which Spark will then see as a categorical variable. The indices start with 0 and are ordered by label frequencies. \n",
    "\n",
    "Three steps to implementing the ```StringIndexer```\n",
    "1. Build the StringIndexer model: specify the input and output col name\n",
    "2. Learn the StringIndexer model: fit the model with your data\n",
    "3. Execute the indexing: call transform to execute the indexing \n",
    "\n",
    "Immediately after ```StringIndexer```, we follow up with ```OneHotEncoder```. ```OneHotEncoder``` converts each category of a String Indexed column with a sparse vector. \n",
    "\n",
    "Let's look at a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----------+\n",
      "|Day of the week|weather|temperature|\n",
      "+---------------+-------+-----------+\n",
      "|         Monday|  Sunny|         80|\n",
      "|         Friday| Cloudy|         70|\n",
      "|        Tuesday|   Snow|         40|\n",
      "|       Thursday|   Rain|         60|\n",
      "|         Friday|   Rain|         50|\n",
      "+---------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'Day of the week':['Monday','Friday','Tuesday','Thursday','Friday'],\n",
    "        'weather':['Sunny','Cloudy','Snow','Rain','Rain'],\n",
    "        'temperature':['80','70','40','60','50'],\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----------+-----------+\n",
      "|Day of the week|weather|temperature|indexed_day|\n",
      "+---------------+-------+-----------+-----------+\n",
      "|         Monday|  Sunny|         80|        1.0|\n",
      "|         Friday| Cloudy|         70|        0.0|\n",
      "|        Tuesday|   Snow|         40|        3.0|\n",
      "|       Thursday|   Rain|         60|        2.0|\n",
      "|         Friday|   Rain|         50|        0.0|\n",
      "+---------------+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#build indexer\n",
    "string_indexer = StringIndexer(inputCol='Day of the week',outputCol='indexed_day')\n",
    "\n",
    "#learn the model\n",
    "string_indexer_model = string_indexer.fit(df)\n",
    "\n",
    "#transform the data\n",
    "df_stringindexer = string_indexer_model.transform(df)\n",
    "\n",
    "df_stringindexer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use ```OneHotEncoder```, which follows the same steps as detailed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----------+-----------+-------------+\n",
      "|Day of the week|weather|temperature|indexed_day|  encoded_day|\n",
      "+---------------+-------+-----------+-----------+-------------+\n",
      "|         Monday|  Sunny|         80|        1.0|(3,[1],[1.0])|\n",
      "|         Friday| Cloudy|         70|        0.0|(3,[0],[1.0])|\n",
      "|        Tuesday|   Snow|         40|        3.0|    (3,[],[])|\n",
      "|       Thursday|   Rain|         60|        2.0|(3,[2],[1.0])|\n",
      "|         Friday|   Rain|         50|        0.0|(3,[0],[1.0])|\n",
      "+---------------+-------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "OneHotEncoder(inputCols=['indexed_day'],outputCols=['encoded_day']).fit(df_stringindexer).transform(df_stringindexer).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of ```oneHotEncoder``` is read as \\[ vector size, \\[index of the variable\\], \\[1.0\\]\\]. It represents a binary sparse matrix. Note that, by default, Spark drops the last category to ensure linear independence. Before training a model, we should OneHotEncode all categorical data and merge them using ```VectorAssembler```, as demonstrated in [PySpark in Action: Will I Respond Correctly?](#PySpark-in-Action:-Will-I-Respond-Correctly?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark pipeline\n",
    "\n",
    "Pipeline is a sequence of stages which consist of estimators and/or transformers. It allows us to fit and transform continuously without saving the intermediate products.\n",
    "In the code below, we pipelined the string-indexing and one-hot-encoding stage of the two categorical variables-\"Day of the week\" and \"weather\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------+-----------+\n",
      "|onehotencoded_Day of the week|onehotencoded_weather|temperature|\n",
      "+-----------------------------+---------------------+-----------+\n",
      "|                (4,[1],[1.0])|        (4,[3],[1.0])|         80|\n",
      "|                (4,[0],[1.0])|        (4,[1],[1.0])|         70|\n",
      "|                (4,[3],[1.0])|        (4,[2],[1.0])|         40|\n",
      "|                (4,[2],[1.0])|        (4,[0],[1.0])|         60|\n",
      "|                (4,[0],[1.0])|        (4,[0],[1.0])|         50|\n",
      "+-----------------------------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "categorical_columns = ['Day of the week', 'weather']\n",
    "stringindexer_stages = [StringIndexer(inputCol = column,outputCol='stringindexed_'+column)\\\n",
    "                        for column in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(dropLast=False,inputCol = 'stringindexed_'+column,outputCol='onehotencoded_'+column)\\\n",
    "                        for column in categorical_columns]\n",
    "\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "\n",
    "#build pipeline model\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "#fit and transform\n",
    "df_coded = pipeline.fit(df).transform(df)\n",
    "#only select interested columns\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['temperature']\n",
    "df_coded = df_coded.select(selected_columns)\n",
    "df_coded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ML package in Spark\n",
    "\n",
    "Spark also offers a very intuitive machine learning API that is similiar to Scikit-Learn. Please check the MLlib API for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting off a SparkContext\n",
    "\n",
    "A SparkContext needs to be manually shutoff before opening a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark in Action: Will I Respond Correctly?\n",
    "\n",
    "In the sections below, we provide an example using Spark DataFrame and MLlib to predict which words are more likely to be responded to correctly during a lexical decision task, based on their length, frequency, and whether it is an outlier word. We will using Spark to read in the data for data preprocessing, create StringIndexing, OneHotEncoding and vectorAssembling and train a logistic regression model with cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[X: int, Sub_ID: int, Trial: int, Type: int, D_RT: string, D_Word: string, Outlier: boolean, D_Zscore: double, Correct: int, Length: int, Log_Freq_HAL: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "# start a sparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sparkContext=sc)\n",
    "\n",
    "#read in data\n",
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"joined_Lexical_Data.csv\")\n",
    "\n",
    "#cache for faster access\n",
    "data.cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract the features and labels from the DataFrame. In this dataset, the relevant features are Outlier, Length, Log_Frequency_Hal. The label is Correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#select relevant data and rename the columns\n",
    "selected_data = data.select(col('`Log_Freq_HAL`').alias('Frequency'),'Outlier','Length', 'Correct');\n",
    "\n",
    "#change relevant columns types to float\n",
    "df = selected_data.withColumn('Fequency',selected_data.Frequency.cast('float'))\n",
    "df = selected_data.withColumn('Length',selected_data.Length.cast('float'))\n",
    "df = selected_data.withColumn('Correct',selected_data.Correct.cast('float'))\n",
    "#cast boolean column to numeric column\n",
    "df = df.withColumn('Outlier',when(col('Outlier')=='True',1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pre-process the categorical varaible Outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+---------+-------+\n",
      "|onehotencoded_Outlier|Length|Frequency|Correct|\n",
      "+---------------------+------+---------+-------+\n",
      "|            (1,[],[])|     6|    8.856|    1.0|\n",
      "|        (1,[0],[1.0])|    11|    4.644|    1.0|\n",
      "|        (1,[0],[1.0])|     7|    8.304|    1.0|\n",
      "|            (1,[],[])|     9|    2.639|    1.0|\n",
      "|        (1,[0],[1.0])|     8|    1.386|    1.0|\n",
      "|        (1,[0],[1.0])|     8|    5.268|    1.0|\n",
      "|        (1,[0],[1.0])|    10|    9.339|    1.0|\n",
      "|        (1,[0],[1.0])|     6|     4.19|    1.0|\n",
      "|        (1,[0],[1.0])|    11|      0.0|    1.0|\n",
      "|        (1,[0],[1.0])|     9|    5.537|    1.0|\n",
      "|        (1,[0],[1.0])|     6|    5.557|    1.0|\n",
      "|        (1,[0],[1.0])|     6|   10.041|    1.0|\n",
      "|            (1,[],[])|    11|    6.392|    1.0|\n",
      "|        (1,[0],[1.0])|    13|    5.011|    1.0|\n",
      "|            (1,[],[])|     8|    5.817|    1.0|\n",
      "|            (1,[],[])|    12|    7.474|    1.0|\n",
      "|        (1,[0],[1.0])|     5|     3.85|    1.0|\n",
      "|        (1,[0],[1.0])|    11|     6.23|    1.0|\n",
      "|            (1,[],[])|     7|    6.387|    1.0|\n",
      "|            (1,[],[])|     8|    4.248|    1.0|\n",
      "+---------------------+------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create onehotencoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "categorical_columns = list(set(['Outlier']))\n",
    "#build stages\n",
    "stringindexer_stages = [StringIndexer(inputCol = column,outputCol='stringindexed_'+column)\\\n",
    "                        for column in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol = 'stringindexed_'+column,outputCol='onehotencoded_'+column)\\\n",
    "                        for column in categorical_columns]\n",
    "\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "\n",
    "#build pipeline model\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "#fit pipeline model\n",
    "pipeline_mode = pipeline.fit(df)\n",
    "\n",
    "#transform data\n",
    "df_coded = pipeline_mode.transform(df)\n",
    "\n",
    "#remove uncoded columns\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['Length', 'Frequency', 'Correct']\n",
    "df_coded = df_coded.select(selected_columns)\n",
    "\n",
    "df_coded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+---------+-------+----------------+-----+\n",
      "|onehotencoded_Outlier|Length|Frequency|Correct|        features|label|\n",
      "+---------------------+------+---------+-------+----------------+-----+\n",
      "|            (1,[],[])|     6|    8.856|    1.0| [0.0,6.0,8.856]|  1.0|\n",
      "|        (1,[0],[1.0])|    11|    4.644|    1.0|[1.0,11.0,4.644]|  1.0|\n",
      "|        (1,[0],[1.0])|     7|    8.304|    1.0| [1.0,7.0,8.304]|  1.0|\n",
      "|            (1,[],[])|     9|    2.639|    1.0| [0.0,9.0,2.639]|  1.0|\n",
      "|        (1,[0],[1.0])|     8|    1.386|    1.0| [1.0,8.0,1.386]|  1.0|\n",
      "|        (1,[0],[1.0])|     8|    5.268|    1.0| [1.0,8.0,5.268]|  1.0|\n",
      "|        (1,[0],[1.0])|    10|    9.339|    1.0|[1.0,10.0,9.339]|  1.0|\n",
      "|        (1,[0],[1.0])|     6|     4.19|    1.0|  [1.0,6.0,4.19]|  1.0|\n",
      "|        (1,[0],[1.0])|    11|      0.0|    1.0|  [1.0,11.0,0.0]|  1.0|\n",
      "|        (1,[0],[1.0])|     9|    5.537|    1.0| [1.0,9.0,5.537]|  1.0|\n",
      "|        (1,[0],[1.0])|     6|    5.557|    1.0| [1.0,6.0,5.557]|  1.0|\n",
      "|        (1,[0],[1.0])|     6|   10.041|    1.0|[1.0,6.0,10.041]|  1.0|\n",
      "|            (1,[],[])|    11|    6.392|    1.0|[0.0,11.0,6.392]|  1.0|\n",
      "|        (1,[0],[1.0])|    13|    5.011|    1.0|[1.0,13.0,5.011]|  1.0|\n",
      "|            (1,[],[])|     8|    5.817|    1.0| [0.0,8.0,5.817]|  1.0|\n",
      "|            (1,[],[])|    12|    7.474|    1.0|[0.0,12.0,7.474]|  1.0|\n",
      "|        (1,[0],[1.0])|     5|     3.85|    1.0|  [1.0,5.0,3.85]|  1.0|\n",
      "|        (1,[0],[1.0])|    11|     6.23|    1.0| [1.0,11.0,6.23]|  1.0|\n",
      "|            (1,[],[])|     7|    6.387|    1.0| [0.0,7.0,6.387]|  1.0|\n",
      "|            (1,[],[])|     8|    4.248|    1.0| [0.0,8.0,4.248]|  1.0|\n",
      "+---------------------+------+---------+-------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#feature columns\n",
    "feature_columns = df_coded.columns[0:3]\n",
    "#build VectorAssembler instance\n",
    "vectorassembler = VectorAssembler(inputCols=feature_columns,outputCol='features')\n",
    "#transform data\n",
    "df_features = vectorassembler.transform(df_coded)\n",
    "df_features = df_features.withColumn('label',col('Correct').alias('label'))\n",
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.6995674611040452\n",
      "testing accuracy:  0.6931035542961131\n"
     ]
    }
   ],
   "source": [
    "#split data into training and test data\n",
    "training, test = df_features.randomSplit([0.8,0.2],seed=100)\n",
    "\n",
    "#cross-validation model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "# here we have 3 values each for regParam and elasticNetParam. So, in total, there are 9 models to train and validate.\n",
    "param_grid = ParamGridBuilder()\\\n",
    "                .addGrid(lr.regParam,[0.0,0.5,1.0])\\\n",
    "                .addGrid(lr.elasticNetParam,[0.0,0.5,1.0])\\\n",
    "                .build()                \n",
    "# we are using 5-fold cv here. \n",
    "cv = CrossValidator(estimator=lr,estimatorParamMaps=param_grid,evaluator=evaluator,numFolds=5)\n",
    "# do cross validation on training dataset\n",
    "cv_model = cv.fit(training)\n",
    "cv_pred = cv_model.transform(training)\n",
    "\n",
    "#use the cv_model to predict test data\n",
    "predictions = cv_model.transform(test)\n",
    "\n",
    "print(\"prediction accuracy: \", evaluator.evaluate(cv_pred))\n",
    "print(\"testing accuracy: \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Spark Cluster\n",
    "\n",
    "So far, we have only scratched the surface of Spark. To fully utilize the parallel processing ability of Spark, you would need to create different clusters and more importantly, configure different clusters. Multiple clusters become a necessity when the data becomes huge. \n",
    "\n",
    "In Spark, there is one master that functions as the cluster manager and the others are workers. The master does the distributing of work and the workers do the actual work. \n",
    "\n",
    "In general, there are 3 ways to start a cluster: standalone, Hadoop YARN, Apache Mesos. In the following section, we will only be exploring the standalone option. This means that there are multiple machines (that have Spark installed) function as clusters. The master is provided by Spark itself. We will be starting a cluster on our local machine, so our computer will function both as a master and as workers. Now, this is not very useful but it will give you a flavor of Spark clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be safe, first check that you have java installed:\n",
    "    \n",
    "    $ java -version\n",
    "    \n",
    "Before we start, we need to download a separate version of Spark.\n",
    "\n",
    "    https://www.apache.org/dyn/closer.lua/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "\n",
    "Untar:\n",
    "\n",
    "    $ tar zxvf spark-3.1.1-bin-hadoop2.7.tar\n",
    "\n",
    "Now, cd into ```spark-3.1.1-bin-hadoop2.7/conf```.\n",
    "\n",
    "Write the following in spark-env.sh (e.g. vim spark-env.sh):\n",
    "\n",
    "    export SPARK_WORKER_MEMORY=1g\n",
    "    export SPARK_EXECUTOR_MEMORY=512m\n",
    "    export SPARK_WORKER_INSTANCES=2\n",
    "    export SPARK_WORKER_CORES=2\n",
    "    export SPARK_WORKER_DIR= <some dir to save running logs>\n",
    "\n",
    "After this, type vim workers and write:\n",
    "    \n",
    "    localhost \n",
    "\n",
    "since we only have a standalone machine. If the master is remote, it should be the url of the master.\n",
    "\n",
    "Return to the previous folder and start master with the following command:\n",
    "\n",
    "    $ sbin/start-master.sh\n",
    "\n",
    "Start the workers:\n",
    "\n",
    "    $ sbin/start-workers.sh\n",
    "\n",
    "Note: If you get connection refused error, enable \"Remote Login\" on your machine. \n",
    "\n",
    "Now, go to localhost:8080 using a browser to see the Spark UI.\n",
    "\n",
    "[<img src=\"https://github.com/edenhu11/15388Tutorial/blob/main/sparkUI.png?raw=true\">](https://github.com/edenhu11/15388Tutorial/blob/main/sparkUI.png?raw=true)\n",
    "\n",
    "Now that we have the clusters running, let's open up the shell to run some operations. Unfortunately, it is a scala shell.\n",
    "\n",
    "    $ bin/spark-shell --master URL_displaying_on_the_Left_Corner_of_UI_page\n",
    "\n",
    "[<img src=\"https://github.com/edenhu11/15388Tutorial/blob/main/scala.png?raw=true\">](https://github.com/edenhu11/15388Tutorial/blob/main/scala.png?raw=true)\n",
    "\n",
    "After starting the shell, run these commands and see changes in the UI.\n",
    "\n",
    "    val file=sc.textFile(\"README.md\")\n",
    "    file.count()\n",
    "    file.take(3)\n",
    "\n",
    "Use Ctrl-c to exit the shell. Use\n",
    "\n",
    "    $ sbin/stop-master.sh\n",
    "    $ sbin/stop-workers.sh \n",
    "\n",
    "to stop the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "PySpark API: http://spark.apache.org/docs/3.1.1/api/python/reference/index.html\n",
    "\n",
    "RDD Cheatsheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf\n",
    "\n",
    "DataFrame Cheatsheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "Helpful tutorials with examples:\n",
    "1. https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/ (also recommends relevant books)\n",
    "2. https://github.com/MingChen0919/learning-apache-spark\n",
    "\n",
    "Running PySpark on Amazon AWS: https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
